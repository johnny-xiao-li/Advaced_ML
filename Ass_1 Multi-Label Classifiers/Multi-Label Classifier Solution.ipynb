{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: Multi-label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name(s): Xiao Li\n",
    "\n",
    "Student Number(s): 15206109"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import metrics\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Load the Yeast Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.170975</td>\n",
       "      <td>-0.156748</td>\n",
       "      <td>-0.142151</td>\n",
       "      <td>0.058781</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.197719</td>\n",
       "      <td>0.041850</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>-0.056617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103956</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>-0.098986</td>\n",
       "      <td>-0.054501</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.077933</td>\n",
       "      <td>-0.080529</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509949</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>-0.006411</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119092</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>-0.048207</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>-0.133387</td>\n",
       "      <td>0.068878</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>-0.119784</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>-0.123645</td>\n",
       "      <td>-0.015513</td>\n",
       "      <td>-0.059683</td>\n",
       "      <td>0.091032</td>\n",
       "      <td>-0.043302</td>\n",
       "      <td>0.229219</td>\n",
       "      <td>-0.071498</td>\n",
       "      <td>0.182709</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>0.085327</td>\n",
       "      <td>0.058590</td>\n",
       "      <td>0.085268</td>\n",
       "      <td>-0.020897</td>\n",
       "      <td>0.068972</td>\n",
       "      <td>0.030125</td>\n",
       "      <td>0.078056</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.052618</td>\n",
       "      <td>0.066093</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>0.082526</td>\n",
       "      <td>-0.095571</td>\n",
       "      <td>-0.022019</td>\n",
       "      <td>-0.046793</td>\n",
       "      <td>-0.038360</td>\n",
       "      <td>0.041084</td>\n",
       "      <td>0.056509</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>-0.029657</td>\n",
       "      <td>-0.012198</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>-0.130830</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>-0.009457</td>\n",
       "      <td>-0.058930</td>\n",
       "      <td>-0.041224</td>\n",
       "      <td>0.042269</td>\n",
       "      <td>0.117717</td>\n",
       "      <td>0.037388</td>\n",
       "      <td>-0.085563</td>\n",
       "      <td>0.136649</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2416</th>\n",
       "      <td>-0.171578</td>\n",
       "      <td>-0.066536</td>\n",
       "      <td>0.168206</td>\n",
       "      <td>0.246831</td>\n",
       "      <td>0.079555</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>-0.088908</td>\n",
       "      <td>-0.212926</td>\n",
       "      <td>-0.280230</td>\n",
       "      <td>-0.187064</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2417 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0     0.004168 -0.170975 -0.156748 -0.142151  0.058781  0.026851  0.197719   \n",
       "1    -0.103956  0.011879 -0.098986 -0.054501 -0.007970  0.049113 -0.030580   \n",
       "2     0.509949  0.401709  0.293799  0.087714  0.011686 -0.006411 -0.006255   \n",
       "3     0.119092  0.004412 -0.002262  0.072254  0.044512 -0.051467  0.074686   \n",
       "4     0.042037  0.007054 -0.069483  0.081015 -0.048207  0.089446 -0.004947   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2412 -0.119784  0.001259 -0.123645 -0.015513 -0.059683  0.091032 -0.043302   \n",
       "2413  0.085327  0.058590  0.085268 -0.020897  0.068972  0.030125  0.078056   \n",
       "2414  0.082526 -0.095571 -0.022019 -0.046793 -0.038360  0.041084  0.056509   \n",
       "2415 -0.130830  0.008868 -0.009457 -0.058930 -0.041224  0.042269  0.117717   \n",
       "2416 -0.171578 -0.066536  0.168206  0.246831  0.079555  0.016528 -0.088908   \n",
       "\n",
       "          Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  \\\n",
       "0     0.041850  0.066938 -0.056617  ...       0       0       1       1   \n",
       "1    -0.077933 -0.080529 -0.016267  ...       0       0       0       0   \n",
       "2     0.013646 -0.040666 -0.024447  ...       0       0       0       0   \n",
       "3    -0.007670  0.079438  0.062184  ...       0       0       0       0   \n",
       "4     0.064456 -0.133387  0.068878  ...       1       1       0       0   \n",
       "...        ...       ...       ...  ...     ...     ...     ...     ...   \n",
       "2412  0.229219 -0.071498  0.182709  ...       0       0       0       0   \n",
       "2413  0.011346  0.052618  0.066093  ...       0       0       0       0   \n",
       "2414  0.011749 -0.029657 -0.012198  ...       0       1       1       1   \n",
       "2415  0.037388 -0.085563  0.136649  ...       0       0       0       0   \n",
       "2416 -0.212926 -0.280230 -0.187064  ...       0       0       0       0   \n",
       "\n",
       "      Class9  Class10  Class11  Class12  Class13  Class14  \n",
       "0          0        0        0        1        1        0  \n",
       "1          0        0        0        0        0        0  \n",
       "2          0        0        0        1        1        0  \n",
       "3          0        0        0        0        0        0  \n",
       "4          0        0        0        0        0        0  \n",
       "...      ...      ...      ...      ...      ...      ...  \n",
       "2412       0        0        0        0        0        0  \n",
       "2413       0        0        0        1        1        0  \n",
       "2414       0        0        0        1        1        0  \n",
       "2415       0        0        0        1        1        0  \n",
       "2416       0        0        0        1        1        0  \n",
       "\n",
       "[2417 rows x 117 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file\n",
    "dataSet = pd.read_csv(\"yeast.csv\")\n",
    "dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data set into description feature set and functional classses set.\n",
    "desFeatures = np.array(dataSet.iloc[:,:103])\n",
    "funcClasses = np.array(dataSet.iloc[:,103:])\n",
    "\n",
    "# Obtain all the labels from the functional class\n",
    "func_labels = list(dataSet.columns.values[103:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a213af550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD1CAYAAAC87SVQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de3TU1b338fc33HIUtR4RGxJpNHK4JNHhUkvbI4W0gpC2glkKtCsSgkGLWOBRkYJnmdWUA1ZbbEuKJafYhKcFQUQ56iOkXipdRSHkCXIrATWURJCLGvvAMQSynz9mEic4IZnMJZH5vNbKYmbP/v1+e4vrm82e33zGnHOIiEhsiOvoAYiISPSo6IuIxBAVfRGRGKKiLyISQ1T0RURiiIq+iEgM6drRA2hNr169XHJyckcPQ0TkC2P79u3HnXNXBnqt0xf95ORkysrKOnoYIiJfGGZ2sKXXtL0jIhJDVPRFRGKIir6ISAzp9Hv6IhJ76uvrqa6u5tNPP+3ooXRq8fHxJCUl0a1btzYfo6IvIp1OdXU1l1xyCcnJyZhZRw+nU3LOceLECaqrq7nmmmvafJy2d0Sk0/n000+54oorVPDPw8y44oorgv7XkIq+iHRKKvita89/IxV9EZEWHDlyhEmTJpGSksKgQYMYN24clZWVpKWlhe0aa9euJTU1lbi4uKh8Jkl7+iKxKv+yIPvXRmYcbZA878Wwnq9qcWarfZxzTJgwgSlTprB69WoAKioq+OCDD8I6lrS0NJ599lnuvvvusJ63JVrpi4gE8Nprr9GtWzfuueeepjaPx8PVV1/d9LyqqoqbbrqJIUOGMGTIEP72t78BcPjwYUaMGIHH4yEtLY3Nmzdz9uxZcnJySEtLIz09nSVLlgAwcOBA+vfvH7V5aaUvIm2SXpweVP+dU3ZGaCTRsWvXLoYOHXrePr1796a0tJT4+Hj279/P5MmTKSsr409/+hNjxoxhwYIFnD17llOnTlFRUUFNTQ27du0C4OOPP47GND5HRV9EpJ3q6+uZOXMmFRUVdOnShcrKSgC++tWvkpubS319PePHj8fj8XDttdfy7rvvct9995GZmcno0aM7ZMytbu+Y2QozO2pmu/zanjazCt9PlZlV+NqTzex//F570u+YoWa208wOmNmvTW/Ni0gnlpqayvbt28/bZ8mSJVx11VXs2LGDsrIyTp8+DcCIESN44403SExMJDs7m5KSEi6//HJ27NjByJEjKSws5K677orGND6nLXv6fwBu8W9wzk10znmccx5gHfCs38vvNL7mnLvHr30ZMB3o5/tpdk4Rkc4kIyODuro6ioqKmtq2bdvGwYOfBVjW1taSkJBAXFwcK1eu5OzZswAcPHiQ3r17k5eXx7Rp0ygvL+f48eM0NDSQlZVFQUEB5eXlUZ8TtKHoO+feAD4M9JpvtX4HsOp85zCzBOBS59wW55wDSoDxwQ9XRCQ6zIz169dTWlpKSkoKqamp5Ofn06dPn6Y+M2bMoLi4mOHDh1NZWcnFF18MwOuvv47H42Hw4MGsW7eOWbNmUVNTw8iRI/F4POTk5LBo0SIA1q9fT1JSElu2bCEzM5MxY8ZEdl7eGtxKJ7Nk4AXnXNo57SOAXzrnhvn12w1UAp8ADzvnNpvZMGCxc+47vn43AQ85577b2rWHDRvmlKcvEgFB3rKZfk3foPqH8kbu3r17GThwYLuPjyWB/luZ2fbGunyuUN/InUzzVf5hoK9z7oSZDQWeM7NUIND+fYu/bcxsOt6tIPr2De5/NBERaVm779M3s67AbcDTjW3OuTrn3Anf4+3AO8C/AdVAkt/hScD7LZ3bObfcOTfMOTfsyisDfuOXiIi0QygfzvoO8HfnXHVjg5ldaWZdfI+vxfuG7bvOucPAP81suO99gDuB50O4toiItENbbtlcBWwB+ptZtZlN8700ic+/gTsCeNvMdgDPAPc45xrfBP4R8F/AAbz/Avg/YRi/iIgEodU9fefc5BbacwK0rcN7C2eg/mVA+FKKREQkaMreERGJISr6IiItiEa08oMPPsiAAQO4/vrrmTBhQsQzeZS9IyKdX7Ax0K2er/WY6GhFK998880sWrSIrl278tBDD7Fo0SIeffTRsF7Dn1b6IiIBRCtaefTo0XTt6l1/Dx8+nOrqaiJJK30RkQA6Ilp5xYoVTJw4MSLzaaSiLyLSTuGMVl64cCFdu3blhz/8YUTHrO0dEZEAohmtXFxczAsvvMAf//jHiH8hvIq+iEgA0YpWfvnll3n00UfZsGEDF110UcTnpe0dEZEAGqOVZ8+ezeLFi4mPjyc5OZknnniiqc+MGTPIyspi7dq1jBo1qlm08mOPPUa3bt3o2bMnJSUl1NTUMHXqVBoaGgCaopVnzpxJXV0dN998M+B9M/fJJ58kUtoUrdyRFK0sEiGKVr4gBButrO0dEZEYoqIvIhJDVPRFRGKIir6ISAxR0RcRiSEq+iIiMURFX0SkBdGIVv6P//gPrr/+ejweD6NHj+b991v8+vCw0IezRKTTSy9OD+v52vIZgmhFKz/44IMUFBQA8Otf/5qf/vSnEf1wllb6IiIBRCta+dJLL20638mTJyOevaOVvohIANGMVl6wYAElJSVcdtllvPbaaxGdV6srfTNbYWZHzWyXX1u+mdWYWYXvZ5zfaz8xswNmts/Mxvi13+JrO2Bm88I/FRGR6KqvrycvL4/09HRuv/129uzZA3ijlZ966iny8/PZuXMnl1xySbNo5ZdffrnZCn/hwoUcOnSIH/7whyxdujSiY27L9s4fgFsCtC9xznl8Py8BmNkgYBKQ6jvmt2bWxcy6AIXAWGAQMNnXV0SkU4pmtHKjH/zgB6xbty4i82nUatF3zr0BfNjG890KrHbO1Tnn3gMOADf6fg445951zp0GVvv6ioh0StGKVt6/f3/T+TZs2MCAAQMiOq9Q9vRnmtmdQBlwv3PuIyAReNOvT7WvDeDQOe1fa+nEZjYdmA7Qt29wyX4iIuEQrWjlefPmsW/fPuLi4vjKV74S0Tt3oI3RymaWDLzgnEvzPb8KOA44oABIcM7lmlkhsMU59799/X4PvIT3XxRjnHN3+dqzgRudc/e1dm1FK4tEiKKVLwjBRiu3a6XvnGu6UdXMioAXfE+rgav9uiYBjZ80aKldRESipF336ZtZgt/TCUDjnT0bgElm1sPMrgH6AVuBbUA/M7vGzLrjfbN3Q/uHLSIi7dHqSt/MVgEjgV5mVg08Aow0Mw/e7Z0q4G4A59xuM1sD7AHOAPc65876zjMT2Ah0AVY453aHfTYiInJerRZ959zkAM2/P0//hcDCAO0v4d3fFxGRDqIYBhGRGKKiLyISQ1T0RURaEI1o5UaPP/44Zsbx48fDfm5/ClwTkU5v74Dw3rM/8O97W+0TrWhlgEOHDlFaWhqVD6NqpS8iEkC0opUB5syZw89//vOIxyqDVvoiIgFFK1p5w4YNJCYmcsMNN0R8TqCiLyLSbvX19cycOZOKigq6dOlCZWUl4I1Wzs3Npb6+nvHjx+PxeJpFK2dmZjJ69GhOnTrFwoUL2bRpU9TGrO0dEZEAohGt/M477/Dee+9xww03kJycTHV1NUOGDOHIkSMRm5eKvohIANGIVk5PT+fo0aNUVVVRVVVFUlIS5eXlfPnLX47YvLS9IyISQLSilaM+r7ZEK3ckRSuLRIiilS8IwUYra3tHRCSGqOiLiMQQFX0RkRiioi8iEkNU9EVEYoiKvohIDFHRFxFpQTSilfPz80lMTMTj8eDxeHjppch+waA+nCUinV7hPa+G9Xz3PpnRap9oRivPmTOHBx54IOznDaTVlb6ZrTCzo2a2y6/tMTP7u5m9bWbrzexLvvZkM/sfM6vw/Tzpd8xQM9tpZgfM7NcWjQxREZF2ima0cjS1ZXvnD8At57SVAmnOueuBSuAnfq+945zz+H7u8WtfBkwH+vl+zj2niEinEUy0cnl5OU8//TQ//vGPAZqilSsqKtixYwcej6dZtPLOnTuZOnVq03mWLl3K9ddfT25uLh999FFE59Vq0XfOvQF8eE7bJufcGd/TN4Gk853DzBKAS51zW5w396EEGN++IYuIdA719fXk5eWRnp7O7bffzp49ewBvtPJTTz1Ffn4+O3fu5JJLLmkWrfzyyy9z6aWXAvCjH/2Id955h4qKChISErj//vsjOuZwvJGbC/wfv+fXmNn/NbO/mNlNvrZEoNqvT7WvTUSkU4pGtDLAVVddRZcuXYiLiyMvL4+tW7dGdF4hFX0zWwCcAf7oazoM9HXODQb+F/AnM7sUCLR/32LSm5lNN7MyMys7duxYKEMUEWmXaEQrg3f/v9H69esj8qXr/tp9946ZTQG+C3zbt2WDc64OqPM93m5m7wD/hndl778FlAS839K5nXPLgeXgTdls7xhFRNorWtHKc+fOpaKiAjMjOTmZ3/3ud5GdV1uilc0sGXjBOZfme34L8EvgW865Y379rgQ+dM6dNbNrgc1AunPuQzPbBtwHvAW8BPzGOdfqDamKVhaJEEUrXxCCjVZudaVvZquAkUAvM6sGHsF7t04PoNR35+Wbvjt1RgA/NbMzwFngHudc45vAP8J7J9C/4H0PwP99ABERiYJWi75zbnKA5t+30HcdsK6F18qAyG5WiYjIeSmGQUQkhqjoi4jEEBV9EZEYoqIvIhJDVPRFRFoQjWhlgN/85jf079+f1NRU5s6dG9Zzn0vRyiLS6f1i4nfDer77n36h1T7RilZ+7bXXeP7553n77bfp0aMHR48eDev5z6WVvohIANGKVl62bBnz5s2jR48egDe5M5K00hcRCSCYaOX4+Hj279/P5MmTKSsra4pWXrBgAWfPnuXUqVPNopUBPv74YwAqKyvZvHkzCxYsID4+nscff5yvfvWrEZuXir6ISDvV19czc+ZMKioq6NKlC5WVlYA3Wjk3N5f6+nrGjx+Px+NpFq2cmZnJ6NGjAThz5gwfffQRb775Jtu2beOOO+7g3XffJVLfM6XtHRGRAKIVrZyUlMRtt92GmXHjjTcSFxfH8ePHIzYvFX0RkQCiFa08fvx4Xn3V+x3AlZWVnD59ml69ekVsXtreEREJIFrRyrm5ueTm5pKWlkb37t0pLi6O2NYOtDFauSMpWlkkQhStfEEINlpZ2zsiIjFERV9EJIao6IuIxBAVfRGRGKKiLyISQ1T0RURiiO7Tl5YFeUsf+bWRGYdIBzly5AizZ89m27Zt9OjRo+k+/dtuu60pQydUEydOZN++fYA3j+dLX/oSFRUVYTl3IG0q+ma2AvgucNQ5l+Zr+1fgaSAZqALucM59ZN5PFfwKGAecAnKcc+W+Y6YAD/tO+zPnXHH4piIiF6rqeZvDer6kxTe12ida0cpPP/100+P777+fyy4LcrEVpLZu7/wBuOWctnnAK865fsArvucAY4F+vp/pwDJo+iXxCPA14EbgETO7PJTBi4hESrSilRs551izZg2TJ0+O6LzatNJ3zr1hZsnnNN8KjPQ9LgZeBx7ytZc470d93zSzL5lZgq9vqXPuQwAzK8X7i2RVSDMQEYmAaEUrN9q8eTNXXXUV/fr1i9icILQ9/aucc4cBnHOHzawx+T8ROOTXr9rX1lK7iMgXUjiilRutWrUq4qt8iMzdO4GSgtx52j9/ArPpZlZmZmXHjh0L6+BERNoiWtHK4M3Uf/bZZ5k4cWJE5wShrfQ/MLME3yo/AWj8Ysdq4Gq/fknA+772kee0vx7oxM655cBy8AauhTBGiaL04vSg+ocSyCUSaRkZGcyfP5+ioiLy8vIAb7TyqVOnmvrU1taSlJREXFwcxcXFzaKVExMTycvL4+TJk5SXlzNu3Di6d+9OVlYWKSkp5OTkNJ3nz3/+MwMGDCApKSni8wplpb8BmOJ7PAV43q/9TvMaDtT6toE2AqPN7HLfG7ijfW0iIp1OY7RyaWkpKSkppKamkp+fT58+fZr6zJgxg+LiYoYPH05lZWWzaGWPx8PgwYNZt24ds2bNoqamhpEjR+LxeMjJyWmKVgZYvXp1VLZ2oI3Ryma2Cu8qvRfwAd67cJ4D1gB9gX8AtzvnPvTdsrkU75u0p4Cpzrky33lygfm+0y50zj3V2rUVrdyBOnH0roRBJ/77VbRy2wUbrdzWu3da+hX07QB9HXBvC+dZAaxoyzVFRCT8FMMgIhJDVPRFRGKIir6ISAxR0RcRiSEq+iIiMURFX0SkBUeOHGHSpEmkpKQwaNAgxo0bR2VlJWlpaWG7RkVFBcOHD8fj8TBs2DC2bt0atnMHojx9Een08vPzo36+aEUrz507l0ceeYSxY8fy0ksvMXfuXF5//fWwXsOfVvoiIgFEK1rZzPjkk08Ab6yD/yd+I0ErfRGRAKIVrfzEE08wZswYHnjgARoaGpp+cUSKVvoiIu1UX19PXl4e6enp3H777ezZswfwRis/9dRT5Ofns3PnTi655JJm0covv/wyl156KQDLli1jyZIlHDp0iCVLljBt2rSIjlkrfekwewcEl60y8O97IzQSkc9LTU3lmWeeOW8f/2jlhoYG4uPjgc+ilV988UWys7N58MEHufPOO9mxYwcbN26ksLCQNWvWsGLFCoqLi/nVr34FwO23394scjkStNIXEQkgIyODuro6ioqKmtq2bdvGwYMHm57X1taSkJBAXFwcK1eubBat3Lt3b/Ly8pg2bRrl5eUcP36choYGsrKyKCgooLy8HIA+ffrwl7/8BYBXX321U39zlojIBasxWnn27NksXryY+Ph4kpOTeeKJJ5r6zJgxg6ysLNauXcuoUaOaRSs/9thjdOvWjZ49e1JSUkJNTQ1Tp06loaEBoClauaioiFmzZnHmzBni4+NZvnx5ZOfVlmjljqRo5Q4U4ejdNYvOBNVf2zthpmjlC0Kw0cra3hERiSEq+iIiMURFX0Qkhqjoi4jEEBV9EZEY0u6ib2b9zazC7+cTM5ttZvlmVuPXPs7vmJ+Y2QEz22dmY8IzBRERaat2F33n3D7nnMc55wGGAqeA9b6XlzS+5px7CcDMBgGTgFTgFuC3ZtYltOGLiERONKKVd+zYwde//nXS09P53ve+1xS+Finh+nDWt4F3nHMHzaylPrcCq51zdcB7ZnYAuBHYEqYxiMgF6pVXU8J6vm9nvNNqn2hFK9911108/vjjfOtb32LFihU89thjFBQUhPUa/sJV9CcBq/yezzSzO4Ey4H7n3EdAIvCmX59qX5tESfK8F4PqXxUfoYGIfAG0FK1cVVXV9Lyqqors7GxOnjwJwNKlS/nGN77B4cOHmThxIp988glnzpxh2bJlfOMb32DatGmUlZVhZuTm5jJnzhz27dvHiBEjALj55psZM2ZM5y76ZtYd+D7wE1/TMqAAcL4/fwHkAoH+CRDw48BmNh2YDtC3b3CfAhQRCYdoRSunpaWxYcMGbr31VtauXcuhQ4ciOq9w3L0zFih3zn0A4Jz7wDl31jnXABTh3cIB78r+ar/jkoD3A53QObfcOTfMOTfsyiuvDMMQRUTCLxzRyitWrKCwsJChQ4fyz3/+k+7du0d0zOEo+pPx29oxswS/1yYAu3yPNwCTzKyHmV0D9AMi+2WQIiLtlJqayvbt28/bxz9auaysjNOnTwOfRSsnJiaSnZ1NSUkJl19+OTt27GDkyJEUFhY2RSgPGDCATZs2sX37diZPnkxKSnjfvzhXSNs7ZnYRcDNwt1/zz83Mg3frpqrxNefcbjNbA+wBzgD3OufOhnJ9iS2F97waVP97n8yI0EgkFmRkZDB//nyKiorIy8sDvNHKp06daupTW1tLUlIScXFxFBcXN4tWTkxMJC8vj5MnT1JeXs64cePo3r07WVlZpKSkkJOTA8DRo0fp3bs3DQ0N/OxnP2v2HkIkhFT0nXOngCvOacs+T/+FwMJQrikiEg3RilZetWoVhYWFANx2221MnTo1svNStHLsCP7unR8E1T/S0cqvjiwMqr9W+q1QtPIFQdHKIiLSIhV9EZEYoqIvIhJDVPRFRGKIir6ISAxR0RcRiSEq+iIiLYhGtPLatWtJTU0lLi6Oc29PX7RoEddddx39+/dn48aNYbleuFI2RUQi5suvVYT1fEdGeVrtE61o5bS0NJ599lnuvvvuZu179uxh9erV7N69m/fff5/vfOc7VFZW0qVLaF9DopW+iEgALUUrX331Z7mRVVVV3HTTTQwZMoQhQ4bwt7/9DYDDhw8zYsQIPB4PaWlpbN68mbNnz5KTk0NaWhrp6eksWbIEgIEDB9K/f//PXf/5559n0qRJ9OjRg2uuuYbrrruOrVtDjyvTSl9EJIBoRSu3pKamhuHDhzc9T0pKoqamJuR5qeiLiLRTfX09M2fOpKKigi5dulBZWQl4o5Vzc3Opr69n/PjxeDyeZtHKmZmZjB49+rznDhSRc55vJmwzFX25YP1i4neD6n//0y9EaCTyRZSamsozzzxz3j7+0coNDQ3Ex3u/bq4xWvnFF18kOzubBx98kDvvvJMdO3awceNGCgsLWbNmDStWrGjx3ElJSc2+UKW6upo+ffqEPC/t6YuIBJCRkUFdXR1FRUVNbdu2bePgwYNNz2tra0lISCAuLo6VK1c2i1bu3bs3eXl5TJs2jfLyco4fP05DQwNZWVkUFBRQXl5+3ut///vfZ/Xq1dTV1fHee++xf/9+brzxxvMe0xZa6YuIBBCtaOX169dz3333cezYMTIzM/F4PGzcuJHU1FTuuOMOBg0aRNeuXSksLAz5zh1QtHJMibVo5U8/+mVQ/WNue0fRyhcERSuLiEiLVPRFRGKIir6ISAxR0RcRiSEhF30zqzKznWZWYWZlvrZ/NbNSM9vv+/NyX7uZ2a/N7ICZvW1mQ0K9voiItF24VvqjnHMev3eL5wGvOOf6Aa/4ngOMBfr5fqYDy8J0fRERaYNIbe/cChT7HhcD4/3aS5zXm8CXzCwhQmMQEQlJR0YrnzhxglGjRtGzZ09mzpwZtuuF48NZDthkZg74nXNuOXCVc+4wgHPusJn19vVNBA75HVvtazschnGIyAUq2M+YtKZqcWarfTo6Wjk+Pp6CggJ27drVFNIWDuFY6X/TOTcE79bNvWY24jx9A6UFfe7TYWY23czKzKzs2LFjYRiiiEhwOjpa+eKLL+bf//3fm/J8wiXklb5z7n3fn0fNbD1wI/CBmSX4VvkJwFFf92rgar/Dk4D3A5xzObAcvJ/IDXWMIiLB6uho5UgJqeib2cVAnHPun77Ho4GfAhuAKcBi35/P+w7ZAMw0s9XA14Daxm0gkS+a/Pz8iPaXzi+S0cqREur2zlXAX81sB7AVeNE59zLeYn+zme0HbvY9B3gJeBc4ABQBM0K8vohIRKSmprJ9+/bz9vGPVi4rK+P06dPAZ9HKiYmJZGdnU1JSwuWXX86OHTsYOXIkhYWF3HXXXdGYxueEtNJ3zr0L3BCg/QTw7QDtDrg3lGuKiERDRkYG8+fPp6ioiLy8PMAbrXzq1KmmPrW1tSQlJREXF0dxcXGzaOXExETy8vI4efIk5eXljBs3ju7du5OVlUVKSgo5OTkdMS1FK4uIBNLR0coAycnJfPLJJ5w+fZrnnnuOTZs2MWjQoJDmpaIvIp1eW26xjIQ+ffqwZs2az7U3vhnbr18/3n777ab2xkI+ZcoUpkyZ8rnjAn1xyoQJE5gwYULA61dVVbVn2Oel7B0RkRiilb6IT/W8zcEdEN7bp0WiQit9EZEYoqIvIhJDVPRFRGKIir6ISAxR0RcRaUFHRiuXlpYydOhQ0tPTGTp0KK+++mpYrqe7d0Sk88u/LMznq221S0dHK/fq1Yv//u//pk+fPuzatYsxY8ZQU1MT8vW00hcRCaCjo5UHDx5Mnz59AG8O0KeffkpdXV3I89JKX0QkgM4Urbxu3ToGDx5Mjx49QpoTqOiLiLRbNKKVd+/ezUMPPcSmTZvCMmZt74iIBNAZopWrq6uZMGECJSUlpKSkhGVeKvoiIgFkZGRQV1dHUVFRU9u2bds4ePBg0/Pa2loSEhKIi4tj5cqVzaKVe/fuTV5eHtOmTaO8vJzjx4/T0NBAVlYWBQUFAcPX/H388cdkZmayaNEivvnNb4ZtXir6IiIBNEYrl5aWkpKSQmpqKvn5+U1vroI3Wrm4uJjhw4dTWVnZLFrZ4/EwePBg1q1bx6xZs6ipqWHkyJF4PB5ycnKaRSsnJSWxZcsWMjMzGTNmDABLly7lwIEDFBQU4PF48Hg8HD169PMDDZL29EWk82vDLZaR0JHRyg8//DAPP/xwu8feEq30RURiiIq+iEgMaff2jpldDZQAXwYagOXOuV+ZWT6QBxzzdZ3vnHvJd8xPgGnAWeDHzrmNIYxdRPwkz3sxqP5V+j6AmBTKnv4Z4H7nXLmZXQJsN7NS32tLnHOP+3c2s0HAJCAV6AP82cz+zTl3NoQxiIhIENq9veOcO+ycK/c9/iewF0g8zyG3Aqudc3XOufeAA8CN7b2+iIgELyx7+maWDAwG3vI1zTSzt81shZld7mtLBA75HVbN+X9JiIhImIV8y6aZ9QTWAbOdc5+Y2TKgAHC+P38B5AIW4HDXwjmnA9MB+vbtG+oQRb6QvvxaRVD9tUUffkeOHGH27Nls27aNHj16kJyczBNPPMFtt93WdNtmqNauXUt+fj579+5l69atDBs2DICtW7cyffp0wJv4mZ+fH/DWzmCFVPTNrBvegv9H59yzvsF94Pd6EfCC72k1cLXf4UnA+4HO65xbDiwHGDZsWMBfDCISO9KL08N6vp1Tdrbap6OjldPS0igrK6Nr164cPnyYG264ge9973t07RraWr3d2ztmZsDvgb3OuV/6tSf4dZsANP463ABMMrMeZnYN0A/Y2t7ri4hEUkdHK1900UVNBf7TTz/FW3JDF8qvjG8C2cBOM2v8d+h8YLKZefBu3VQBdwM453ab2RpgD947f+7VnTsi0ll1hmjlt956i9zcXA4ePMjKlStDXuVDCEXfOfdXAu/Tv3SeYxYCC9t7TRGRziTS0cpf+9rX2L17N3v37mXKlCmMHTuW+PjQ3r3RJ3JFRCNhxfAAAASJSURBVALoDNHKjQYOHMjFF18cljePVfRFRALo6Gjl9957jzNnzjSdb9++fSQnJ4c8L6VsiogE0BitPHv2bBYvXkx8fHzTLZuNZsyYQVZWFmvXrmXUqFHNopUfe+wxunXrRs+ePSkpKaGmpoapU6fS0NAA0Cxa+b777uPYsWNkZmbi8XjYuHEjf/3rX1m8eDHdunUjLi6O3/72t/Tq1Svkeanoi0TJK68G+c1Hti4yA/kCasstlpHQkdHK2dnZZGdnt3vsLdH2johIDFHRFxGJISr6IiIxREVfRCSGqOiLiMQQFX0RkRiioi8i0oIjR44wadIkUlJSGDRoEOPGjaOyspK0tLSwXWPt2rWkpqYSFxdHWVnZ517/xz/+Qc+ePXn88ccDHB083acvIp3e3gEDw3q+gX/f22qfjo5WbjRnzhzGjh0btutppS8iEkBHRysDPPfcc1x77bWkpqaGbV5a6YuIBNDR0conT57k0UcfpbS0NGxbO6CiLyLSbpGMVn7kkUeYM2cOPXv2DOuYtb0jIhJAR0crv/XWW8ydO7cp5O0///M/Wbp0acjz0kpfRCSAjIwM5s+fT1FREXl5eYA3WvnUqVNNfWpra0lKSiIuLo7i4uJm0cqJiYnk5eVx8uRJysvLGTduHN27dycrK4uUlBRycnLOe/3Nmzc3Pc7Pz6dnz57MnDkz5HlppS8iEkBjtHJpaSkpKSmkpqaSn59Pnz59mvrMmDGD4uJihg8fTmVlZbNoZY/Hw+DBg1m3bh2zZs2ipqaGkSNH4vF4yMnJaRatnJSUxJYtW8jMzGTMmDERnZdW+iLS6bXlFstI6MhoZX/5+fnBDPu8tNIXEYkhUV/pm9ktwK+ALsB/OecWR3sMIhJ5wX6gqqNW87Emqit9M+sCFAJjgUHAZDMbFM0xiIjEsmhv79wIHHDOveucOw2sBm6N8hhE5AvAOdfRQ+j02vPfKNpFPxE45Pe82tcmItIkPj6eEydOqPCfh3OOEydOEB8fH9Rx0d7TtwBtn/tbNbPpwHTf0/9nZvsiOioJKNBf1vntCqp30Pt6+74d7BFBeYAXgz2kF3A8AkPxGRy5U9MJ/37tsxFdeeWVXRcuXJicnJz8L2bBjzQWOOeoqqr6nwULFlQdO3bszDkvf6Wl4yyav0nN7OtAvnNujO/5TwCcc4uiNgiRMDGzMufcsI4eh0gwor29sw3oZ2bXmFl3YBKwIcpjEBGJWVHd3nHOnTGzmcBGvLdsrnDO7Y7mGEREYllUt3dELiRmNt05t7yjxyESDBV9EZEYohgGEZEYoqIvIhJDlLIp0kZmNgDvJ8gT8X6+5H1gg3NOoTHyhaGVvkgbmNlDeGNDDNiK9/ZjA1aZ2byOHJtIMPRGrkgbmFklkOqcqz+nvTuw2znXr2NGJhIcrfRF2qYB6BOgPcH3msgXgvb0RdpmNvCKme3ns9DAvsB1QOhfXCoSJdreEWkjM4vDGw+eiHc/vxrY5pw726EDEwmCir6ISAzRnr6ISAxR0RcRiSEq+iIiMURFX0Qkhqjoi4jEkP8PPOPkkP+LK5EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show each class count number\n",
    "classes = dict()\n",
    "for class_ in range(funcClasses[0].size):\n",
    "    key = \"Class\" + str(class_ + 1)\n",
    "    value = dataSet.iloc[:, 103 + class_].value_counts()[1]\n",
    "    classes[key] = [value]\n",
    "graph = pd.DataFrame.from_dict(classes)\n",
    "graph.plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement the Binary Relevance Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Binary relevance algorithms\n",
    "class BinaryRelevance():\n",
    "    \n",
    "    # Constructor: input the classifer and the labels of the functional class\n",
    "    def __init__(self, classifier, labels):\n",
    "        self.models = dict()\n",
    "        self.labels = labels\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    # Fit function, train the classifier\n",
    "    def fit(self, data_train, func_train):\n",
    "        # the features in all columns, except the class label part\n",
    "        X = data_train\n",
    "        \n",
    "        # Iterate all class labels, create each model\n",
    "        for i in range(len(self.labels)):\n",
    "            \n",
    "            # Obtain the current label\n",
    "            y = np.transpose(func_train[:, i])\n",
    "            \n",
    "            # Use the specific classifier to train the model\n",
    "            self.models[i] = self.classifier().fit(X, y)\n",
    "        return self\n",
    "            \n",
    "    # The prediction function\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Initialized an empty list to store all predictions\n",
    "        pred = list()\n",
    "        \n",
    "        # Iterate all class labels to get each prediction\n",
    "        for i in range(len(self.labels)):\n",
    "            pred.append(self.models[i].predict(X))\n",
    "            \n",
    "        return np.transpose(np.array(pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement the Binary Relevance Algorithm with Under-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Binary relevance algorithms with under-sampling\n",
    "class BinaryRelevance_UnderS():\n",
    "    \n",
    "    # Constructor: input classifier, class labels and the boolean value of whether under-sampling, which true in default \n",
    "    def __init__(self, classifier, labels, underS = True):\n",
    "        self.models = dict()\n",
    "        self.labels = labels\n",
    "        self.classifier = classifier\n",
    "        self.underS = underS\n",
    "        \n",
    "    # Fit function, train the classifier\n",
    "    def fit(self, data_train, func_train):\n",
    "        # Generate an under sampler\n",
    "        underSample = RandomUnderSampler(random_state = 0)\n",
    "        X = data_train\n",
    "        \n",
    "        # Iterate all class labels, create each model\n",
    "        for i in range(len(self.labels)):\n",
    "            \n",
    "            # Obtain the current label\n",
    "            y = np.transpose(func_train[:, i])\n",
    "            \n",
    "            # Whether use the under-sampling\n",
    "            if self.underS:\n",
    "                X_bal, y_bal = underSample.fit_sample(X, y)\n",
    "                \n",
    "            # Use the specific classifier to train the model\n",
    "            self.models[i] = self.classifier().fit(X_bal, y_bal)\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Initialized an empty list to store all predictions\n",
    "        pred = list()\n",
    "        \n",
    "        # Iterate all class labels to get each prediction\n",
    "        for i in range(len(self.labels)):\n",
    "            pred.append(self.models[i].predict(X))\n",
    "        \n",
    "        return np.transpose(np.array(pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Compare the Performance of Different Binary Relevance Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Split the origin dataset in ratio 7:3\n",
    "data_train, data_test, func_train, func_test = train_test_split(desFeatures, funcClasses, random_state = 0, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB classifier without sampling:\n",
      "\n",
      "\t- Hamming Loss:  0.2973238882329791 \n",
      "\n",
      "\t- F1 Score:  0.4441966958169542\n",
      "\n",
      "\n",
      "GaussianNB classifier with sampling:\n",
      "\n",
      "\t- Hamming Loss:  0.37298307752853205 \n",
      "\n",
      "\t- F1 Score:  0.4393360855066418\n"
     ]
    }
   ],
   "source": [
    "# Generate a Binary relevance classifier without sampling\n",
    "br = BinaryRelevance(GaussianNB, func_labels)\n",
    "br.fit(data_train, func_train)\n",
    "func_pred = br.predict(data_test)\n",
    "\n",
    "# Calculate the hamming loss\n",
    "br_hamming_loss = hamming_loss(func_test, func_pred)\n",
    "\n",
    "# Calculate the f1 score\n",
    "br_f1_score = f1_score(func_test, func_pred, average = \"macro\")\n",
    "\n",
    "\n",
    "# Generate a Binary relevance classifier with sampling\n",
    "br_us = BinaryRelevance_UnderS(GaussianNB, func_labels, True)\n",
    "br_us.fit(data_train, func_train)\n",
    "func_us_pred = br_us.predict(data_test)\n",
    "\n",
    "# Calculate the hamming loss\n",
    "br_us_hamming_loss = hamming_loss(func_test, func_us_pred)\n",
    "\n",
    "# Calculate the f1 score\n",
    "br_us_f1_score = f1_score(func_test, func_us_pred, average = \"macro\")\n",
    "\n",
    "print(\"GaussianNB classifier without sampling:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", br_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", br_f1_score)\n",
    "\n",
    "print(\"\\n\\nGaussianNB classifier with sampling:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", br_us_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", br_us_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors classifier without sampling:\n",
      "\n",
      "\t- Hamming Loss:  0.21054702872884692 \n",
      "\n",
      "\t- F1 Score:  0.4171119638846602\n",
      "\n",
      "\n",
      "KNeighbors classifier with sampling:\n",
      "\n",
      "\t- Hamming Loss:  0.3724911452184179 \n",
      "\n",
      "\t- F1 Score:  0.4435257841700581\n"
     ]
    }
   ],
   "source": [
    "# Generate a Binary relevance classifier without sampling\n",
    "br = BinaryRelevance(KNeighborsClassifier, func_labels)\n",
    "br.fit(data_train, func_train)\n",
    "func_pred = br.predict(data_test)\n",
    "\n",
    "# Calculate the hamming loss\n",
    "br_hamming_loss = hamming_loss(func_test, func_pred)\n",
    "\n",
    "# Calculate the f1 score\n",
    "br_f1_score = f1_score(func_test, func_pred, average = \"macro\")\n",
    "\n",
    "\n",
    "# Generate a Binary relevance classifier with sampling\n",
    "br_us = BinaryRelevance_UnderS(KNeighborsClassifier, func_labels, True)\n",
    "br_us.fit(data_train, func_train)\n",
    "func_us_pred = br_us.predict(data_test)\n",
    "\n",
    "# Calculate the hamming loss\n",
    "br_us_hamming_loss = hamming_loss(func_test, func_us_pred)\n",
    "\n",
    "# Calculate the f1 score\n",
    "br_us_f1_score = f1_score(func_test, func_us_pred, average = \"macro\")\n",
    "\n",
    "print(\"KNeighbors classifier without sampling:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", br_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", br_f1_score)\n",
    "\n",
    "print(\"\\n\\nKNeighbors classifier with sampling:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", br_us_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", br_us_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression classifier without sampling:\n",
      "\n",
      "\t- Hamming Loss:  0.2085792994883904 \n",
      "\n",
      "\t- F1 Score:  0.3361452699253122\n",
      "\n",
      "\n",
      "LogisticRegression classifier with sampling:\n",
      "\n",
      "\t- Hamming Loss:  0.3737701692247147 \n",
      "\n",
      "\t- F1 Score:  0.4354239044216425\n"
     ]
    }
   ],
   "source": [
    "# Generate a Binary relevance classifier without sampling\n",
    "br = BinaryRelevance(LogisticRegression, func_labels)\n",
    "br.fit(data_train, func_train)\n",
    "func_pred = br.predict(data_test)\n",
    "\n",
    "# Calculate the hamming loss\n",
    "br_hamming_loss = hamming_loss(func_test, func_pred)\n",
    "\n",
    "# Calculate the f1 score\n",
    "br_f1_score = f1_score(func_test, func_pred, average = \"macro\")\n",
    "\n",
    "\n",
    "# Generate a Binary relevance classifier with sampling\n",
    "br_us = BinaryRelevance_UnderS(LogisticRegression, func_labels, True)\n",
    "br_us.fit(data_train, func_train)\n",
    "func_us_pred = br_us.predict(data_test)\n",
    "\n",
    "# Calculate the hamming loss\n",
    "br_us_hamming_loss = hamming_loss(func_test, func_us_pred)\n",
    "\n",
    "# Calculate the f1 score\n",
    "br_us_f1_score = f1_score(func_test, func_us_pred, average = \"macro\")\n",
    "\n",
    "print(\"LogisticRegression classifier without sampling:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", br_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", br_f1_score)\n",
    "\n",
    "print(\"\\n\\nLogisticRegression classifier with sampling:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", br_us_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", br_us_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Classifier Chains algorithms\n",
    "class ClassifierChains():\n",
    "    \n",
    "    # Constructor: input the classifer and the labels of the functional class\n",
    "    def __init__(self, classifier, labels):\n",
    "        self.models = dict()\n",
    "        self.labels = labels\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    # Fit function, train the classifier\n",
    "    def fit(self, data_train, func_train):\n",
    "        \n",
    "        # Select the train set from the feature set\n",
    "        X = data_train[:, :103]\n",
    "        \n",
    "        # Iterate all class labels, create each model\n",
    "        for i in range(len(self.labels)):\n",
    "            y = np.transpose(func_train[:, i])\n",
    "            self.models[i] = self.classifier().fit(X, y)\n",
    "            \n",
    "            # Chain here, to connect X and y\n",
    "            X = np.c_[X, y]\n",
    "            \n",
    "        return self\n",
    "           \n",
    "    # The prediction function\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Initialized an empty list to store all predictions\n",
    "        pred = list()\n",
    "        \n",
    "        # Iterate all class labels to get each prediction\n",
    "        for i in range(len(self.labels)):\n",
    "            pred.append(self.models[i].predict(X[:, :103+i]))\n",
    "        \n",
    "        return np.transpose(np.array(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Evaluate the Performance of the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate a new data set\n",
    "data_cc = np.array(dataSet)\n",
    "\n",
    "# Slipt the train set and test set from the feature and class labels\n",
    "data_train_cc, data_test_cc, func_train_cc, func_test_cc = train_test_split(data_cc, funcClasses, random_state = 0, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB classifier with Classifier Chain:\n",
      "\n",
      "\t- Hamming Loss:  0.21861471861471862 \n",
      "\n",
      "\t- F1 Score:  0.5664085982787397\n",
      "\n",
      "\n",
      "GaussianNB classifier without sampling:\n",
      "\n",
      "\t- Hamming Loss:  0.2973238882329791 \n",
      "\n",
      "\t- F1 Score:  0.4441966958169542\n",
      "\n",
      "\n",
      "GaussianNB classifier with sampling:\n",
      "\n",
      "\t- Hamming Loss:  0.37298307752853205 \n",
      "\n",
      "\t- F1 Score:  0.4393360855066418\n"
     ]
    }
   ],
   "source": [
    "# Generate a Classifier Chain classifier\n",
    "cc = ClassifierChains(GaussianNB, func_labels)\n",
    "cc.fit(data_train_cc, func_train_cc)\n",
    "func_pred_cc = cc.predict(data_test_cc)\n",
    "\n",
    "# Calculate the hamming loss\n",
    "cc_hamming_loss = hamming_loss(func_test_cc, func_pred_cc)\n",
    "\n",
    "# Calculate the f1 score\n",
    "cc_f1_score = f1_score(func_test_cc, func_pred_cc, average = \"macro\")\n",
    "\n",
    "\n",
    "# Generate a Binary relevance classifier without sampling\n",
    "br = BinaryRelevance(GaussianNB, func_labels)\n",
    "br.fit(data_train, func_train)\n",
    "func_pred = br.predict(data_test)\n",
    "\n",
    "# Calculate the hamming loss\n",
    "br_hamming_loss = hamming_loss(func_test, func_pred)\n",
    "\n",
    "# Calculate the f1 score\n",
    "br_f1_score = f1_score(func_test, func_pred, average = \"macro\")\n",
    "\n",
    "\n",
    "# Generate a Binary relevance classifier with sampling\n",
    "br_us = BinaryRelevance_UnderS(GaussianNB, func_labels, True)\n",
    "br_us.fit(data_train, func_train)\n",
    "func_us_pred = br_us.predict(data_test)\n",
    "\n",
    "# Calculate the hamming loss\n",
    "br_us_hamming_loss = hamming_loss(func_test, func_us_pred)\n",
    "\n",
    "# Calculate the f1 score\n",
    "br_us_f1_score = f1_score(func_test, func_us_pred, average = \"macro\")\n",
    "\n",
    "print(\"GaussianNB classifier with Classifier Chain:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", cc_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", cc_f1_score)\n",
    "\n",
    "print(\"\\n\\nGaussianNB classifier without sampling:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", br_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", br_f1_score)\n",
    "\n",
    "print(\"\\n\\nGaussianNB classifier with sampling:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", br_us_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", br_us_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In task 5, with gaussian bayes classifer, the hamming loss in mehtod classifer chain is much lower than traditional binary relevance classifier and F1 score is the highest in the above three classifier. Compare with the sampleing method and non-sampling method, the sampling one has lower hammming loss, while the f1 score is close. So, in the task 3, we use 3 different classifier to test the smapling and non-sampling. The result shows that , sampling method has much higher F1 score and lower hamming loss.\n",
    "\n",
    "print(\"GaussianNB classifier without sampling:\\n\")\n",
    "print(\"\\t- Hamming Loss: \", br_hamming_loss, \"\\n\")\n",
    "print(\"\\t- F1 Score: \", br_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
