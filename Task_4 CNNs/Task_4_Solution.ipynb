{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification from images with VGG-16\n",
    "\n",
    "In this example we build a clasification model to recognise differnet kinds of images. This example uses limited data and is inspired by the keras blog post \"Building powerful image classification models using very little data\" from https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html. This example also uses a VGG-16 pre-trained model fine tuned for a specifc problem.\n",
    "\n",
    "The datasets used are from kaggle \n",
    "* cats and dogs https://www.kaggle.com/c/dogs-vs-cats\n",
    "* monkeys https://www.kaggle.com/slothkong/10-monkey-species/home \n",
    "* flowers https://www.kaggle.com/alxmamaev/flowers-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop, adam\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import sklearn\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import csv\n",
    "import os \n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import scipy as sp\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the data set process. We provide the name of a folder contianing images. There are two assumptions:\n",
    "* The images of differnet classes are contained in folders with class ids\n",
    "* A csv file called class_labels exists which is a map from class ids to class names (for printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset name\n",
    "#dataset_name = 'flowers'\n",
    "#dataset_name = 'cats_and_dogs'\n",
    "dataset_name = '10-monkey-species'\n",
    "\n",
    "\n",
    "# assuming data is contained in a train and a validation set\n",
    "train_data_dir = dataset_name + '/train/'\n",
    "\n",
    "# Set up some parmaeters for data loading\n",
    "sample_rate = 1.0\n",
    "\n",
    "# desired dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# different backends (e.g. tensorflow and theano) use different orderings for image data - fix this!\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class training\n",
      "Processed 1 of 0 for class training \n",
      "Data shape: (0, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "training_class_folders = [i for i in os.listdir(train_data_dir) if not (i.startswith('.') or i.startswith('Icon'))] # use this for full dataset\n",
    "num_classes = len(training_class_folders)\n",
    "\n",
    "# Initialise arrays for data storage\n",
    "X_data = np.ndarray((0, input_shape[0], input_shape[1], input_shape[2]), dtype=np.float)\n",
    "y_data= np.ndarray(0, dtype=np.str)\n",
    "    \n",
    "# Loop through the class folders\n",
    "for i, image_cls in enumerate(training_class_folders):\n",
    "    \n",
    "    \n",
    "    print('Processing class {}'.format(image_cls))\n",
    "    image_class_folder = train_data_dir + image_cls + \"/\"\n",
    "\n",
    "    # generate filenames from the data folder and do sampling\n",
    "    image_filenames = [image_class_folder+i for i in os.listdir(image_class_folder) if not i.startswith('.')] # use this for full dataset\n",
    "    image_filenames = [i for i in image_filenames if (i.endswith(\".jpg\") or i.endswith(\".png\") or i.endswith(\".jpeg\"))]  # Only process image files\n",
    "    image_filenames = random.sample(image_filenames, int(len(image_filenames)*sample_rate))\n",
    "\n",
    "    # Create a data array for image data\n",
    "    count = len(image_filenames)\n",
    "    X_data_part = np.ndarray((count, input_shape[0], input_shape[1], input_shape[2]), dtype=np.float)\n",
    "\n",
    "    # Iterate throuigh the filenames and for each one load the image, resize and normalise\n",
    "    for i, image_file in enumerate(image_filenames):\n",
    "        \n",
    "        # Low the images and resize them\n",
    "        image = cv2.imread(image_file, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (img_height, img_width), interpolation=cv2.INTER_CUBIC)        \n",
    "        image = image[:,:,[2,1,0]] # OpenCV and matplotlib use differnet channel oerderings so fix this\n",
    "        \n",
    "        # If channel order of network does not match open cv format swap it\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            image=np.swapaxes(np.swapaxes(image, 1, 2), 0, 1)\n",
    "            \n",
    "        # Add image data to data array and normalise\n",
    "        X_data_part[i] = image\n",
    "        X_data_part[i] = X_data_part[i]/255\n",
    "        \n",
    "        # Add label to label array\n",
    "        y_data = np.append(y_data, image_cls)\n",
    "        \n",
    "        if i%100 == 0: print('Processed {} of {} for class {} '.format(i, count, image_cls))\n",
    "\n",
    "    print('Processed {} of {} for class {} '.format(i + 1, count, image_cls))\n",
    "    \n",
    "    # Append the part to the overall data array\n",
    "    X_data = np.append(X_data, X_data_part, axis=0)\n",
    "    \n",
    "print(\"Data shape: {}\".format(X_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=0.7, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c090ea9e94a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perfrom split to train, validation, test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0;32m-> 2122\u001b[0;31m                                               default_test_size=0.25)\n\u001b[0m\u001b[1;32m   2123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             \u001b[0;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[0;32m-> 1805\u001b[0;31m                                                 train_size)\n\u001b[0m\u001b[1;32m   1806\u001b[0m         )\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=0.7, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Perfrom split to train, validation, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=0, test_size = 0.30, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape output data for use with a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_train_num = y_train_encoder.fit_transform(y_train)\n",
    "y_train_wide = keras.utils.to_categorical(y_train_num, num_classes)\n",
    "\n",
    "y_test_num = y_train_encoder.fit_transform(y_test)\n",
    "y_test_wide = keras.utils.to_categorical(y_test_num, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the number to label mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_num_label = dict()\n",
    "\n",
    "for idx, lbl in enumerate(y_train_encoder.classes_):\n",
    "    classes_num_label[idx] = lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a few randomly sampled example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=4\n",
    "row_images = 5\n",
    "col_images = 5\n",
    "plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n",
    "\n",
    "for i in range(row_images * col_images):\n",
    "    i_rand = random.randint(0, X_train.shape[0])\n",
    "    plt.subplot(row_images,col_images,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(PIL.Image.fromarray(((X_train[i_rand]) * 255).astype(np.uint8)))\n",
    "    plt.title((str(i_rand) + \" \" + y_train[i_rand]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "vgg16_model = keras.applications.VGG16(weights='imagenet', include_top=False, input_shape = X_train[0].shape)\n",
    "display(vgg16_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_last_layer = vgg16_model.output\n",
    "\n",
    "# build a classifier model to put on top of the VGG16 model\n",
    "x1 = Flatten()(vgg16_last_layer)\n",
    "x2 = Dense(256, activation='relu')(x1)\n",
    "x3 = Dropout(0.5)(x2)\n",
    "final_layer = Dense(num_classes, activation = 'softmax')(x3)\n",
    "\n",
    "# Assemble the full model out of both parts\n",
    "full_model = keras.Model(vgg16_model.input, final_layer)\n",
    "\n",
    "# moving over weights from a pre-trained smaller model specifically for our problem might help rather than random initialisation.\n",
    "#top_weights_filepath = './best_weights_notebook22.hdf5'\n",
    "#old_model = keras.models.load_model(top_weights_filepath)\n",
    "#full_model.layers[-1].set_weights(old_model.layers[-1].get_weights())\n",
    "\n",
    "# set the first 17 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in full_model.layers[:17]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "full_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. We allow the model to overfit but save the best set of weights based on validation loss as we go. Then at the end of training (assuming the model has overfit) we revert back to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "# Set up the callback to save the best model based on validaion data - notebook 2.2 needs to be run first.\n",
    "best_weights_filepath = './best_weights_notebook23.hdf5'\n",
    "mcp = ModelCheckpoint(best_weights_filepath, monitor=\"val_loss\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "history = full_model.fit(X_train, y_train_wide,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.2,\n",
    "          shuffle=True,\n",
    "          callbacks=[mcp])\n",
    "\n",
    "#reload best weights\n",
    "#model.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "pred = np.argmax(full_model.predict(X_train),axis=1)\n",
    "\n",
    "# Print performance details\n",
    "print(metrics.classification_report(y_train_num, pred))\n",
    "print(\"Confusion matrix\")\n",
    "print(metrics.confusion_matrix(y_train_num, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "pred = np.argmax(full_model.predict(X_test),axis=1)\n",
    "\n",
    "# Print performance details\n",
    "print(metrics.classification_report(y_test_num, pred))\n",
    "print(\"Confusion matrix\")\n",
    "print(metrics.confusion_matrix(y_test_num, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=4\n",
    "row_images = 4\n",
    "col_images = 4\n",
    "\n",
    "maxtoshow = row_images * col_images\n",
    "predictions = pred.reshape(-1)\n",
    "corrects = predictions == y_test_num\n",
    "ii = 0\n",
    "plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    if ii>=maxtoshow:\n",
    "        break\n",
    "    if corrects[i]:\n",
    "        plt.subplot(row_images,col_images, ii+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(PIL.Image.fromarray(((X_test[i]) * 255).astype(np.uint8)))\n",
    "        plt.title(\"{} for {}\".format(classes_num_label[predictions[i]], y_test[i]))\n",
    "        ii = ii + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw some examples of mis-classifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=4\n",
    "row_images = 4\n",
    "col_images = 4\n",
    "\n",
    "maxtoshow = row_images * col_images\n",
    "predictions = pred.reshape(-1)\n",
    "errors = predictions != y_test_num\n",
    "ii = 0\n",
    "plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    if ii>=maxtoshow:\n",
    "        break\n",
    "    if errors[i]:\n",
    "        plt.subplot(row_images,col_images, ii+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(PIL.Image.fromarray(((X_test[i]) * 255).astype(np.uint8)))\n",
    "        plt.title(\"{} for {}\".format(classes_num_label[predictions[i]], y_test[i]))\n",
    "        ii = ii + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
